Notes 2019-01-09

SSIM:
- image_diff.py

Arya results:
- SSIM increases (good) as resolution decreases
 - perhaps due to 11x11 window, which does not encapsulate a whole character
- zoom matters a lot
 - match pixel placement (in evaluation) as accurately as possible
 - SSIM: 0.10900802597770408 without matching zoom
 - SSIM: 0.1038874548691809 for a *totally different image*
 - SSIM: 0.35508744840872164 with matching zoom
- matching brightness / contrast matters a little bit
 - SSIM: 0.35508744840872164 without matching levels
 - SSIM: 0.40035811574407315 with matching levels

Typed character results:
- 0 and 8: SSIM: 0.6497579054533621
- 0 and #: SSIM: 0.23633129442480139
- seems to work fairly well

MNIST annoy ANNs:
- distance correlates well with SSIM
- but MUCH faster. Time for entire batch of 60000 images to be compared:
    - skSSIM: 17.00658041199995
    - annDist: 0.0645328509999672
    - 263 times faster!
    - see charts
    - not sure if this is an accurate comparison: redo later with real-world scenario

ANN provides good approximation of SSIM, itself a good approximation of MOS (mean opinion score)


2019-01-10

blend_modes is very slow for compositing
- Instead, let's represent white as 0 and black as 255. (inverted)
- Most "black" areas of a character will be around 200.
- If we add them together (if two characters overlap at a pixel), we will get 255 (clamped)
 - So an assumption is made that 2 overlapping characters are as dark as 100 overlapping characters
- Matrix addition is fast and requires no padding step (just add matrix regions where they overlap)

Todo:
0. Terminology
    "char": specific MxN pixel matrix representing a single typed character (1 row/column exactly)
    "char vector": same as char but flattened to 1xM*N. these are stored in Annoy model

    "photo" / "input image" / "source image": image to be converted into typable
     - photo should be resized / padded to be a multiple of charWidth/2 x charHeight/2, min size 2 x 2 chars
    "typable": the output of the algorithm. M'xN' matrix of indices for chars (number rows, number columns)
    "mockup": composite image of chars in typable
     - goal is to make _mockup_ resemble _photo_ as closely as possible

    "pixel grid" / "photo grid": pixel coordinates in photo
    "char grid": positions of chars in relation to pixel grid


    (later...)
     - ex. if char.shape is (20,32), [20,32] on the pixel grid would be [2,2] on the char grid
       - Why not [1,1]? Because of the double overlap, char at [1,1] is at pixel [10,16]

    "overlap": multiple typed layers, offset by a certain amount
    "horizontal overlap": page is shifted charWidth/2 to the left compared to original
    "vertical overlap": page is shifted charHeight/2 down compared to original
    "double overlap": 4 overlapping layers, [ [TL, TR], [BL, BR] ]


2. Sanity check on ANN selection of images
 - No overlapping, scan through input image left to right with a certain window size (matching the char size)
 - Use a generated photo containing only chars, positioned on char grid
 - Create typable and mockup, compare with original (both visually and SSIR)


2019-01-11

Using ANN selection alone doesn't seem to work very well. Only considers shape, not tone
- See hemingway_annoy.png

- Verify that chopping, ANN select are working properly (robustly)
 - Verify math is correct (write formulas by hand)
    - Math is correct, but roundtrip increases lightness! (WHY?)
    - See hemingway1, hemingway1m, hemingway1m2
    - FIXED! it was matplotlib's imsave(). Using cv2.imwrite() instead

 - end to end system, reconstructing the same image as used for charset
  - should work perfectly when rowLength == slicesX
   - up to the point at which slices become so small they lose shape information

 - Make diagram of software model

- Test other skimage selection algorithms: SSIM, PSNR, NRMSE, MSE
 - done. Output from PSNR, NRMSE, MSE all identical - use MSE only
 - Compare with different distance metrics in Annoy
  - dot and hamming don't work
  - euclidean is same as MSE
  - manhattan very similar to MSE, just more contrasty
  - angular is similar to SSIM as seen previously (with Text charset)
  - SSIM is more similar to MSE than any NN selection with images
  - adding more Trees to Annoy makes no difference (default 10 is fine)
 - Combination of Annoy:Angular and MSE (select best MSE from top K NN)
  - see mockup_80_combo_best*.png
  - top 3 seems to be a good value
  - lower values will be lighter shade
  - this might work well for combining separate layers (ie. multiple passes)
  - Try: using MSE on dark and/or low-contrast (shapeless) areas and NN/combo on high-contrast (shapely) areas
  - high contrast areas can be selected in a similar way to Li priority dither
 - Try: implement FS and Li dither (broad)

- Implement tests on entire system (input image (to both charset and target) vs mockup)

Notation:
- change in step or change in dimension: delta or Q?
 - https://math.stackexchange.com/questions/44771/is-there-an-analogue-to-the-delta-symbol-for-ratios


2019-01-13

Current state:
- Many options exposed in selector_with_modes.py:
   - it relies only on prep_charset_simple.py
    Usage: python simple_selector.py sourceImg targetImg slicesX slicesY rowLength mode [distance mode] [bestK]
    sourceImg, targetImg are file paths
    slicesX, slicesY are positive integers
    rowLength is best set to same as slicesX, for testing reconstruction of the same sourceImg vs targetImg
    Mode can be: nn, ssim, mse, combo.
        If mode is nn, distance mode can be: "angular", "euclidean", "manhattan", "hamming", or "dot". 
        If mode is combo, bestK specifies how many 'best k' nearest neighbours are given to the MSE evaluator.

- Updates in terminology to reflect change towards generic photo testing scenarios (not typewriter specific)
    "char / slice": specific MxN pixel matrix representing a single character / input image slice  (1 row/column exactly)
    "char vector / slice vector": same as char but flattened to 1xM*N. these are stored in Annoy model

    "source image / input image / charset image": image to be sliced into chars / slices
    "target image / photo": image to be converted into typable
     - photo should be resized / padded to be a multiple of charWidth/2 x charHeight/2, min size 2 x 2 chars
    "chosen slices / typable": the output of the algorithm. M'xN' matrix of indices for chars /  (number rows, number columns)
    "output image / generated image / mockup": composite image of chosen slices
     - goal is to make _mockup_ resemble _photo_ as closely as possible

    "pixel grid": pixel coordinates (x, y) of individual pixels in an image
    "char grid / input grid": positions of chars / slices in source image.
    "target grid": positions of chars / slices in output image
    "slice area / region": a specific slice in the input or target grid

    (later...)
     - ex. if char.shape is (20,32), [20,32] on the pixel grid would be [1,1] on the char grid

    "overlap": multiple typed layers, offset by a certain amount
    "horizontal overlap": page is shifted charWidth/2 to the left compared to original
    "vertical overlap": page is shifted charHeight/2 down compared to original
    "double overlap": 4 overlapping layers, [ [TL, TR], [BL, BR] ]

Paths forward:
- Normalize targetimage to match black/white levels of source image (charset)
 - fixes MSE preference towards darkest character at all times
- MSE selection on shapeless areas and NN on high-contrast shapely areas
 - can just adjust the "bestK" parameter in combo mode, separately for each area
 - the local contrast can be found for each slice area by Li dither algorithm
- Try implementing the broad dither (brighten/darken entire slice areas to correct)


2019-01-14

Current state:
- Usage remains the same.
    - Only combo mode is worth using, practically speaking. Just set the bestK param to 1 if you want the pure NN selection.
- Normalization is done in matchContrast()
    - generates levelAdjustedChars
    - has a hidden hyperparameter, darkenLevel, which controls how much black level is normalized

Todo:
- Implement broad dither (FS, Li)
 - Q: where is dither applied? on target image?
- Vary bestK by image contents (once Li priority dither implemented)


2019-01-15

Questions for prof:
- Discuss results so far (these notes)
 - SSIM vs ANN

- Matching contrast between source and target (average levels for slice?)
 - clipping issues
- Dither: how to apply? On source or target slices?
 - Does storing it in the ditherMap make sense?
- Evaluation functions:
 - Something better than MSE for tone matching?
 - Varying bestK by image contents?
  - MSE selection on shapeless areas and NN on high-contrast shapely areas
- Overall evaluation:
 - SSIM, PSNR ok?
- Recommendations for GUI?
- Performance improvements?
 - Multiple ANN indices, one for angular, one for euclidean or manhattan?
 - Faster blend_modes?

- Make diagram of software model


2019-01-16

- Using a second NN index by Euclidean doesn't work as well as comparing MSE (after selecting kBest from angular metric).
- It is much faster though
- Tried doing a combination of Angular and Euclidean ranks to determine best option. Results are OK.
 - perhaps a combination of scores rather than ranks? but different scales

Current state:
- fiddling with metrics
- fiddling with contrast matching of charset
 - THIS IS IMPORTANT

Todo:
- fix edge problems (instead of padding white, copy last row of pixels)
 - Fixed.
- no more working against Arya image
- work against test images: gradients
 - best way to evaluate contrast matching
   - should be option to normalize against target image
    - darkest source slice becomes as dark as darkest target slice
    - lightest source slice becomes as dark as lightest target slice
   - vs. normalizing only source (recommended)
    - darkest source slice becomes effectively black
    - lightest source slice becomes effectively white
   - vs. no normalization
- evaluation (target vs mockup)


- rewrite using classes:
 - Slice
  - .actualAvg (float)
  - .normalAvg (float)
  - .data (2d array)
 - Photo (can be source or target) ?
  - originalData
  - resizedData
  - amount of padding
  - proportion changes
  - slices?
  - sliceMap (2d array) ('typable')?
   - this would be [[0,1,2],[3,4,5]] etc. initially
  - errorMap (matching typable dimensions)
  - Mockup is also a photo (subclass?)
 - Metric(sourceSlices)
  - uniform interface to different comparisons
   - angular, euclidean NN
   - mse, ssim
   - average
   - combinations of above (nested use)
   - would be nice to have outputs of metrics uniformly scaled...
  - .getSimilar(slice, kBest)
   - slice would need dither applied before being sent to this method
   - getSimilar returns array of k most similar slice indices
 - Mockup(source, target, params, hyperparams)
  - constructor generates mockup
  - .errorMap
  - .sliceMap
  - .dither(idx, error, mode=fs)
   - there is 'error' at 'idx'
   - propagate it out according to 'mode'
  - .data
  - .evaluate('metric')
   - note this does not take Metric object
   - different implementations of evaluation for each 'metric'
   - this can do the cropping and resizing
  - .render()
   - gives back a resized image made from slices
    - proportions corrected
  - .renderCrop()
   - renders, then crops out the padding to match source image
   - called by .evaluate()


Dicts:
 - params = { sourceFn, targetFn, srcSlices(x,y), rowLength }
 - hyperparams = { mode, ditherMode, kBest, etc }


Photo has slices and slicesMap
Target must be resized in a special way
And mockup must be resized in the inverse of that
Where should the "chopCharset" be implemented?


2019-01-20

Preparing for meeting this week:
- Matching contrast between source and target (average levels for slice?)
  * - IDEA: match target levels to source:
  * raise measured black level of target (lowest avg. of target slice) to lowest avg. of source slice.
        diff = (minAvgSourceSlice - minAvgTargetSlice)
        target = (target * (255-diff)/255) + diff
        A: Modify the image (either source or target) that has more midtone variation
         - in my case, this is never the "source" if it is a typed character set
- Dither: storing it in the ditherMap make sense? YES.
- Evaluation functions:
 - Something better than MSE for tone matching? NO
 * Varying bestK by shapeliness? YES.
 * - Instead of bestK, shapeliness dictates the weight given to scores for Euclidean vs Angular distance
   - DONE (but shapeliness a manual hyperparamater)

- Compositing:
 * Faster blend_modes?
 - A: even just adding is fine (low priority to make this better)
 - A: Alpha to represent ink: 0 is no ink, 1 is full ink (full black)
  - multiply by 1-alpha.
  - "alpha blend modes"
  - DONE

- Tests:
- Overall evaluation:
 - SSIM, PSNR ok? YES. Implement.
 * SSIM vs Angular NN, MSE vs Euclidean NN correlations
  - on different datasets:
   - MNIST, typed chars, faces
   - with members of same dataset, or others.
   - can show clusters with T-SNE, etc. (not from ANN model necessarily)
   - how to show from knowing only distances? (ex. from ANN models)
    - input all points distance to all other points ie
          p1  p2  p3  p4
     p1   .1  .2  .5  .2
     p2   .2  .8  .6  .3  
     p3   .5  .6  .8  .1
     p4   .2  .3  .1  .7
     Then use dimensionality reduction to get into a nicer embedding.

- How to order selections:
  A:
- characterizing edges to determine order of character selection
 - gradient magnitude (like contrast, but not a ratio)
  - can approx by taking sumSq(diffX, diffY)
  - can simply sum the gradient magnitude of slices (integrate)
  - look in textbooks. "sobel filter"

2019-01-23

combo generation is slow, but working well
- using shrunken images:
 - try different resampling
 - building NN models is very slow when dimensionality high; this helps a lot

angular vs. euclidean metric blending works

to fix:
- everything :P
- use .ann files to save ALL information required
 - so combos don't need to be regenerated and subsequent runs instant
- constraints on selection
- this error: (at rowLength 70)
    File ".\gen_combos.py", line 141, in <module>
        m = genMockup(t, combos, (target.shape[1], target.shape[0]), targetPadding)
    File "C:\Users\Jules\sliced-image-collage\selector_helpers.py", line 142, in genMock
    up
        resized = cv2.resize(mockup, dsize=targetShape, interpolation=cv2.INTER_CUBIC)
    cv2.error: OpenCV(3.4.5) C:\projects\opencv-python\opencv\modules\imgproc\src\resize.c
    pp:3784: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'


improve:
- contrast matching (just implement formula in notes)
- build GUI asap
- full-res rendering
- padded slices used for combo generation, and full-res rendering

then:
- order the selection process (prioritize shapeliness, etc. of slices)
- apply dither during selection (shade error propagated to adjacent slices)
- multi-pass selection: TL, BL for shape, and once selected: TR, BR for shade.
 - dither can apply to one or both passes
- dither (fine):  ??? NO
  - A: use Li dither with small radius (1 "subslice" pixel)
    - A: priority dither with a Queue should be fine, 


Decisions:
- Multi-pass selection
 - first pass selected from only 2 overlaps (TR, BR constrained to blank)
    - selects for shape
 - second pass constrains TL, BL to those chosen in first pass
    - selects for tone
- How many overlaps in a combo
- How is spill factored into weighing
 - Can we use partial spill, ie. 0.5 of the adjacent quadrant?
   So the total width of a quadrant (+ spill) is 1 complete charWidth

Issues:
- Having a large number of combos stored in NN model requires more trees (>10) to be effective. With a small number of combos (4096) it works fine but with a large number (millions) there becomes a lot of selection errors
 - A: Use a semi-random subset including 4 of a kind, lightest, darkest, etc. up to a limit that works. include 4 of same character moved around
  - equivalence class: so you can substitute out a similar combo (that isn't in NN model) to fulfil constraints
  - experiment: find out how many is a practical max for NN model

Other selection mechanisms:
- Select 1 character, add it to mockup.
 - 2nd pass, 3rd, 4th. Always add to mockup and compare composite to source slice.
 - 
 - But no (initial) consideration of overlap


2019-01-28

Starting rewrite.
What needs to be saved? ANN models (2). Charset object - image and params. Combo images.


Selecting with a large combo vs 
* Calculate how much darker (on average) overlaps are:

0 1 0
2 3 2
0 1 0

0: No overlap
1: Horizontal overlap
2: Vertical overlap
3: Double overlap

Get averages for each of these 4 categories for charset.

The source image has already been processed to match darkest double_overlap
Then, the source slice should be processed (before comparison) s.t.:
slice_area_0 *= avg_no_overlap / avg_double_overlap
slice_area_1 *= avg_h_overlap / avg_double_overlap
slice_area_2 *= avg_v_overlap / avg_double_overlap
slice_area_3 *= 1

We could take this one step further, where

1 2 3
4 5 6
7 8 9

1: [
    [_, _],
    [_, x]
   ]
2: [
    [_, _],
    [x, x]
   ]
3: [
    [_, _],
    [x, _]
   ]
4: [
    [_, x],
    [_, x]
   ]
5: [
    [x, x],
    [x, x]
   ]
6: [
    [x, _],
    [x, _]
   ]
...

Wait, we can actually get these averages as we generate the combos, by slicing the image in place! So we will have all these averages, and the *darkest* center average, by the end of combo generation.


Problem:

So let's say we have selected some sections, but not others. We will soon end up in a situation where we are choosing an area which has different constraints. With the ANN we can't store both the unconstrained AND constrained groups of 9 areas...

Let's say that 6 and 9 have been already chosen by block ABCDEFGHI.

1 2 3
4 5 A B C
7 8 D E F
    G H I

(Note that we are really only choosing 1, 2, 4, 5 if we consider the top left corner of the character to reside in those sub-slices)

Instead of having only 2 pieces of overlap information for 6 (A), we now have 3 since A has been chosen already, and also 3 overlaps for 9 (D) since both A and D are chosen. We'd like to take advantage of this extra information, but if we are storing all 9 sub-slices as one vector in the ANN model, we can't.

Solution? Store only the middle quadrant in ANN (as we have been). But how do we then come up with a total score? But storing smaller slices (ie. just a quarter of a character) reduces ability to shape-match.

We *could* have several different ANN models with different sizes stored, for instance the full 9, and then just 1.

Could compare the performance / quality of selecting 9 sub-slices independently, vs. the entire set together.

b1 b2 b3
b4 b5 b6
b7 b8 b9

We have to find the selection for b5 that maximizes sum(b1...b9):

for bx in [b1...b9]:
 bx = getBestNN()

for b in b5:
 spillScore = sum(max(b in bx where b conforms to constraints imposed by b5) for bx in [b1...b9])

How long will this take? It would allow for complete information to be used at every selection!!! (within the range of the 9 sub-slices)
It also won't require any of that compensation for some areas being limited in how dark they can be.


2019-01-29

constrained combo version working
- but very slow
- accuracy not great with random ordered selection

1. speed it up
- read through to find easy fixes
    - Selector.getSimilar should be profiled:

- profiling different functions
- ditch NN and go constraint based (MSE, SSIM)

2. get priority of slices to evaluate
- sobel filter

3. "9 sub-slices" selection window


2019-01-30

Results from speed tests:
    Initialization (per combo):
        ComboSet(charset=charset): 0.25ms / combo (to generate combos)
        Selector(comboSet): 0.64 ms / combo (to build 2 ANN models)
        ComboGrid(rows, cols): 0 ms / combo

        For 10k combos:
            2.5s to generate
            6.4s to build ANN
            8.9s total

    Selection (per subslice - same dimension as combo):
        getConstraints(row, col): 0.0036ms / subslice
                                  0.0025ms with speedups
        getSimilar(slice): 25ms / subslice with 10k combos and 2 ANN
                           8.5ms with only one ANN for 10k combos
                           180ms with 160k combos - ouch!
        matchesConstraints(constraints): 0.0004ms / combo
            so for 10k combos, 0.4ms/subslice
        
        For 10k subslices (full page) with 10k combos:
            0.036s to get constraints
            250s for 2 ANN selection 
            or 85s for 1 ANN selection
            4s to match constraints

        Goal: 100ms vs current 100s: 1000x too slow!
        Vast majority of time is for ANN selection (99.96%)

The ANN selection is shockingly slow.
SSIM is 263 (lets say 250) times slower, but how many comparisons can we avoid?
Considering we select from left to right, top to bottom, each comparison needs to be for numChars**k, where k is:

4 3 3 3 3
3 2 2 2 2
2 2 2 2 2
2 2 2 2 2

As the number of subslices under consideration grows, the number of comparisons per subslice approaches numChars**2 (only 100, vs 10000 when numChars=10). This is 100x reduction. If numChars=20, it is a 400x reduction. ie, a reduction of numChars**2.

We can further speed up the selection when using SSIM (or another selector which is compatible with constraining the comparisons) by using representatives. This could reduce the size of the character set further (unclear how much - need to investigate).

2019-01-31

Todo:
    - contrast matching
     - DONE (source compared to true black, not to target image)

    - prioritize selection order
     - DONE (laplace, seems good enough)

    - selector using SSIM, MSE
    - constraint based selection (4d array of combos)
     - DONE but slow as hell
      - SSIM is useless on a very small window (subslice)

    - dither (Li)
     - allows for priority selection
     - DONE
     - DONE (residuals) "double application" of dither? and/or track residuals
        - no need for double application since residuals very small (due to lightening of target img)
    
    ACCURACY:
    - consider spill
     - "lookahead" to consider spill with full information
    
    PERFORMANCE:
    - how much can combo set be reduced with representatives / bins?
     - measure similarity between combos, this could be done with ANN
     - loop through combos matching constraints and check distance to chosen representative
     - surely representatives help less and less as the constraints increase?
     - would random (or partly random) sampling be more useful that representatives for when few constraints?
     - put bounds on error so that we can stop comparing when we found something good enough

    Eventually:
    - GUI
    - High res rendering (from original charset)


2019-02-01

More results:
- Shrinking:
    - Should probably do *after* combos generated (TODO along with hi-res rendering)
    - Shrink of 16 is too much (produces a subslice only 1x2 pixels - vs 2x2 used in 2016 version)
    - Shrink of 8 yields 2x4 which is a decent approximation
    - For shrinking: INTER_AREA is good. All others are bad
    - Shrinking changes things, not necessarily better or worse (between 1 and 8)
- Dither is a mixed bag
 - TODO Something is broken... see _wtf.png
- Prioritization improved by blurring the Sobel filtered image before summing slices
- Overall the results are worse than using the ANN
- blending angular distance (of flattened vector) with MSE helps?

TODO:
 - show results as image is being generated (for fun, debugging)
    - DONE! Bugs found and fixed. Order of evaluation of array indices.. ugh
 - Consider spill (how?)
    - Needs to be done ASAP because other 
 - Update ANN selector to work with new interface (swappable)
    - Generic selector with many options
        - Priority: linear, alternating scan, random, priority, inner->outer, outer->inner, vertical, or specify priority in matrix of combogrid size
        - MSE, SSIM, Angular, Euclidean-ANN, Angular-ANN, AvgLevel
            - Or blend between
 - Fix Dither bug (?)
 - Fix preview bug (?)
 - Bins / representatives
 - Bounds for error
 - Multiple passes
 - A* selection, etc (move outward from seed points)


Actually... dead ending with this combo selection.
Basically, it is selecting 4 characters at one time.
The only way this could be good is if it looks at the area under all 4 characters
    - store a larger image, and compare that: better, but incomplete information when constraints already imposed (ie, when this 4 character chunk overlaps with already chosen chars)
    - OR composite on the fly: could be additionally to pre-composited combos, when there are more overlaps (wouldn't take so long to composite when there are fewer characters to choose - and can composite over closest existing combo)
    - each of these strategies looks at 9 subslices
Another approach would be to look at 4 subslices (quadrants), under 1 chosen character.
    - need to think about this, seems like a LOT of combos.

A compeletely new path would return to the simplicity of selecting one character at a time, rather than 4.
- Simplest sanity test: go back to angular NN one layer version
 - Do 4 passes, choosing the best single character on each pass
 - Can composite as you go, or just treat each layer independently

Benefits: Angular selection is more likely to commit "sins of omission" rather than "sins of comission" which is preferable especially for this strategy.
 - Subsequent passes could focus on a different metric (improving shading, dither...)


2019-02-03

Modeling the way a human would draw a portrait
- start with lines (SSIM, angular selection - sins of omission)
- shading
- fill in more lines
- more shading?

Different comparisons:
- Whole character (4 subslice)
- Whole character (4 subslice, plus best in each "unknown" overlap)
	-   
	123
	4A5
	678
	- 8 unknown chars which overlap with char we are choosing (A)


- 4 character combo (1 subslice)
- 4 character combo (9 subslices)
- 4 character combo (9 subslices, plus best in each "unknown" subslice)
- 6 or 9 character combo

"Do no harm" selection.
Increasing the evaluation window every pass? (slow, but it IS fine-tuning..)
- avoid "local maxima" - bad overall shape at the expense of good local shape (within a slice)


ComboGrid is laid out exactly the same as before, except now we select 4 combo grid locations at a time (but with 1 char).

So each set of TL,TR,BL,BR (4 subslices) representing a typed character has a shapeliness (priority). This is the sum of the priority of the underlying subslices. The shapeliness of that underlying subslice determines which metric (or what blend) should be used for that selection.
Note that the order in which selections are made doesn't make much difference in this scheme, other than selecting which layer (which offset) is used. But we should select all of 1 layer at a time.

Images of each character / combination in 4 quadrants *can be* pre-composited, that is, each subslice is pre-composited and fast matrix tiling can reassemble them to represent any possibility. That way, only the normal numChars**4 combos need to be generated, just like now.

*pre-generating the combos may not be necessary or feasible. It depends on the performance penalty of compositing on the fly vs number of selected characters (or representatives) used to generate combos. For instance, with 10,000 character image (full page) and 100 chars to choose from (full set), compositing on the fly would take 268 seconds, while generating before would take 26800 (100 times longer).
    - these calculations based on results in speed_tests.py
    - BEST: do compositing on the fly, but store results as you go. check each time if already composited.

Thus I think moving ahead with compositing on the fly is best. The mockup will be updated rather than re-generated each time (new function). WorkingMockup should be a class, with a single instance belonging to a Generator. 

So, in order of priority (why not?)
Each typed character (any layer):
    Each candidate character (or representative):
        Composite the character (in place) into TempWorkingMockup.
        Compare the composite under that character to the target image
         - use MSE and/or SSIM and/or custom metric to get score
    Put chosen character into ComboGrid and update WorkingMockup.

This will yield complete information about the ink we are adding, and on average 5/8 (0.625) information (.25+.5+.75+1)/4 about all the ink, on the first pass
Note that we have more information after every selection, so the final selections (if each layer chosen in order) would have complete information, although the previous selections would be fixed. So we have complete information, but partial choice (25%, at each selection).

 vs. with the one subslice that has full overlap, 4 with double overlap, and 4 with single = 4/9 (0.444_) information. We have 100% choice at first selection, but on average 50% choice, and in that case there are ... mehhh. It's just so much easier to consider one at a time.

After considering one at a time, we will have a complete ComboGrid. We can then iterate on improving areas of high importance, etc by changing 1 character at a time, starting back at the first selection - repeating the same selection order as before. If no characters in a subslice change through this second iteration, that subslice can be marked "stable". We can repeat this process until all subslices have stabilized.

Note that this method only considers the shape or shade matching within a particular character (4 subslices).
Further iterations could widen the window to select more than 1 character at a time - perhaps this should be specified manually (since very slow). Then error can be computed on that entire region (which is not the same for SSIM as the sum of the error between the sub-regions).
We can also widen the measurement window while still allowing only 1 change at a time.

All of these methods are prone to finding local maxima.


2019-02-05 Meeting with prof

Progress:
- Constraints working perfectly (after much error!)
- New general idea: "do no harm" / "sins of omission" selection

- Selection order:
    - Linear causes error to be "smeared" to the right, bottom
    - Random is worse than linear
    - Priority is hit and miss
        - Having trouble getting a good priority map with Laplace (Sobel)
    - User could specify selection order

- Selectors:
    - NN selector can't be constrained, so all results returned
        - Must then filter through to get constraints
        - This is very slow!

- Speed tests
    - 0.25ms / combo subslice to generate (composite, object init)
        - So 1ms per entire character during selection
    - getSimilar with single ANN: 8.5ms for 10k combos
        - 180ms for 160k combos!!!
        - selection via constraints means numChars**2 instead of numChars**4 comparisons. So even with 250x slower selection (SSIM vs NN) this still wins


Issues:
- High amount of error due to not considering "spill"
    - Current selection process is a terrible idea!
- Dither is really screwed up! Need to start over?
    - In practice there are no residuals. Should we limit the black level in the dither image (target, which has been lightened?)
    - Quite confused about dither implementation, generally
- SSIM is useless on a small window (subslice)
    - How can I use a larger window?
    - Works OK on 4 subslices (as seen in first version of program)

Ideas:
- combos to generate ahead of time:
    - each character alone
    - all combinations of darkest k characters
        - to find "black level" for level matching target image
- "lookahead" when selecting chars
    - what is math for this
- upper bound on error: any selection below this is acceptable
 - A don't worry about this
- Some kind of search algorithm starting at a seed point?
- Multiple passes (with some constraints fixed)
    - for 1 character at a time, loop back to 1st layer once all selected
- Different metrics for different passes? (How to improve shading once shape is good? How to improve dither on shading passes?)

Other questions:
- representatives / bins
    - still makes sense?
    - just sort them a do a binary search until desired error reached?
    - low-res representation? (ie. avg value for each subslice)
        [TL, TR, BL, BR]
        - would be very fast
- finding what needs to be added? (ie. subtract existing choices from target image?)


2019-02-06

Working on the rewrite
- more character-level information
- optional shrink before compositing
- use full character set, composite on the fly
- find darkest combos to get black level
    - store combos in sparse array (4 dimensional defaultdict)
    - add more combos as they are composited on the fly


2019-02-07

Rewrite is going very well!

General thoughts:
- Selecting one character at a time is a very sane approach
- Multiple passes:
    - Taking multiple passes (at the same grid alignment ie TL, TR, BL, BR)
    - Improves both contrast range AND detail
        - The key is not to add too much ink at once
        * Need to modify metric to seriously discourage "sins of commission"
    - DONE Pass a configuration to the generator and have it run all passes at once
- Metrics:
    - mixing metrics (numerically) doesn't work as well as one might expect
    - MSE is a very solid metric
    - SSIM can be used early in the process
        - early on it has a tendency to be too light
        - but later has a tendency to get too dark
    - Metrics are OK going forward, no need to improve

* Main things to work on:
    - Dither (how?)
        - broad dither most important
            - if happening in isolation on multiple layers / overlaps...
            - should distribute itself into a finer pattern
    * Improving on previous selections
        - After selecting a new layer, move 1 layer back and try 'next best' chars
        * After selecting all 4 layers, move back to first layer and cycle back
            - Only need to continue forward if something has changed!
            - Can repeat this indefinitely if not careful
    * Evaluation of output images (SSIM and MSE)
    - GUI
        - Adjust hyperparameters
        - Scroll through for different matches (per layer) ?
        - Deepening search: "Locked" areas and "Unlocked" areas
        - Select which characters to include
        - Show area scores on hover


2019-02-08

Todo:
- Dither

- DONE Looping back to previous selections
    * And continuing only where selection changes
        - "Jiggle" algorithm
        - Randomize order?
    - For now, consider layer "fixed" if already onto next overtype layer
        * Unlock this to loop back to previous pass (no strong concept of overtype)
            - would need the comboGrid to store multiple Indices in each TL,TR,BL,BR
            - Ability to jiggle after every new layer, not just every pass

    - Ability to first select with MSE, then correct with SSIM, then correct with MSE, etc

- Evaluate output images / itermediate layers with MSE, SSIM
- Use shrunken images for comparisons, generally.
- Time the process and count comparisons

- Better command line options (or better yet, GUI)

Updates to terminology:
- "Pass": All 4 offset layers
- "Layer": One of TL, TR, BL, BR
- "Char location": Combogrid location in which char is BR char.
- "Overtyped char": List of chars selected for a char location.

2019-02-10

Finding great references today, especially "Structure based ascii art"
 - new metric worth trying AISS

TODO:
 - Implement dirty bit algorithm

cool algorithm for selection:
- select a random char and place it.
- select the first character that improves things and place it (overtop).
- repeat for other 2 layers. then back at the top, looking for replacement.
(i'm already doing this, but with the "best" character each time).

i thought this would be more like a "jiggle" towards stability
(some local optima will emerge no matter what i do)

variations:
- first random char meeting a tolerance, or "best of first x random chars"
- or starting in a sorted list of chars at the average value to fill
 - then binary search

 USAGE:
python kword.py .\hemingwayCrop.png 20 bbb 2 show
Arguments are:
    <target image path>
    <row length>
    <mode for each pass (blend, mse, ssim)> 
    <number adjustments to optimize first pass>
    <show (or no argument to save animation)>